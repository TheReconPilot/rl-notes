<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">Multi-Armed Bandits | Reinforcement Learning Notes</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://thereconpilot.github.io/rl-notes/tabular-methods/multi-armed-bandits"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Multi-Armed Bandits | Reinforcement Learning Notes"><meta data-react-helmet="true" name="description" content="The Concept"><meta data-react-helmet="true" property="og:description" content="The Concept"><link data-react-helmet="true" rel="shortcut icon" href="/rl-notes/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://thereconpilot.github.io/rl-notes/tabular-methods/multi-armed-bandits"><link data-react-helmet="true" rel="alternate" href="https://thereconpilot.github.io/rl-notes/tabular-methods/multi-armed-bandits" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://thereconpilot.github.io/rl-notes/tabular-methods/multi-armed-bandits" hreflang="x-default"><link rel="stylesheet" href="/rl-notes/assets/css/styles.0db5a581.css">
<link rel="preload" href="/rl-notes/assets/js/runtime~main.60cd991a.js" as="script">
<link rel="preload" href="/rl-notes/assets/js/main.0b7ea4ad.js" as="script">
</head>
<body>
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/rl-notes/"><div class="navbar__logo"><img src="/rl-notes/img/logo.svg" alt="Site Logo" class="themedImage_1VuW themedImage--light_3UqQ"><img src="/rl-notes/img/logo.svg" alt="Site Logo" class="themedImage_1VuW themedImage--dark_hz6m"></div><b class="navbar__title">Reinforcement Learning Notes</b></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/TheReconPilot/rl-notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_71bT toggle_3Zt9 toggleDisabled_3cF-"><div class="toggleTrack_32Fl" role="button" tabindex="-1"><div class="toggleTrackCheck_3lV7"><span class="toggleIcon_O4iE">ðŸŒœ</span></div><div class="toggleTrackX_S2yS"><span class="toggleIcon_O4iE">ðŸŒž</span></div><div class="toggleTrackThumb_xI_Z"></div></div><input type="checkbox" class="toggleScreenReader_28Tw" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_31aa"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_35hR" type="button"></button><aside class="docSidebarContainer_3Kbt"><div class="sidebar_15mo"><nav class="menu thin-scrollbar menu_Bmed"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rl-notes/">Reinforcement Learning</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#">Tabular Methods</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/rl-notes/tabular-methods/multi-armed-bandits">Multi-Armed Bandits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rl-notes/tabular-methods/mdp">Markov Decision Process</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rl-notes/tabular-methods/cross-entropy">Cross Entropy Method</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Rewards and GPI</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Model-free methods</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Approximate Solution Methods</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rl-notes/resources">Resources</a></li></ul></nav></div></aside><main class="docMainContainer_3ufF"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><div class="tocCollapsible_1PrD theme-doc-toc-mobile tocMobile_3Hoh"><button type="button" class="clean-btn tocCollapsibleButton_2O1e">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Multi-Armed Bandits</h1></header><h2 class="anchor anchorWithStickyNavbar_31ik" id="the-concept">The Concept<a aria-hidden="true" class="hash-link" href="#the-concept" title="Direct link to heading">â€‹</a></h2><ul><li><code>k</code> different options, or actions at each step</li><li>A reward given at each action, depending on a stationary probability distribution</li><li><strong>Objective:</strong> Maximize total reward over a given number of steps, say 500 steps.</li></ul><blockquote><p>The name comes from imagining a gambler at a row of slot machines (sometimes known as &quot;one-armed bandits&quot;), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.</p><p>-<!-- --> Wikipedia</p></blockquote><p><img src="https://blogs.mathworks.com/images/loren/2016/multiarmedbandit.jpg"></p><hr><h2 class="anchor anchorWithStickyNavbar_31ik" id="exploration-vs-exploitation">Exploration vs Exploitation<a aria-hidden="true" class="hash-link" href="#exploration-vs-exploitation" title="Direct link to heading">â€‹</a></h2><table><thead><tr><th><strong>Exploitation</strong></th><th><strong>Exploration</strong></th></tr></thead><tbody><tr><td>Using known-good strategies/actions</td><td>Exploring new unknown strategies/actions</td></tr><tr><td>Known amount of rewards</td><td>Rewards unknown</td></tr></tbody></table><p>If the bandit just exploits the known-strategies, it is possible that they are stuck in a local-optima.</p><p>There may be unknown strategies which yield a higher total reward in the long run.</p><hr><h2 class="anchor anchorWithStickyNavbar_31ik" id="epsilon-greedy-strategy"><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ïµ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">Ïµ</span></span></span></span></span>-greedy strategy<a aria-hidden="true" class="hash-link" href="#epsilon-greedy-strategy" title="Direct link to heading">â€‹</a></h2><p>Exploit known good strategies most of the time, but for an <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ïµ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">Ïµ</span></span></span></span></span> fraction, explore new strategies randomly.</p><p>As the number of steps increases, different actions will be sampled more and more times, eventually leading us to finding the optimal strategy in the limit of very large number of steps.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/rl-notes/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« <!-- -->Reinforcement Learning</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/rl-notes/tabular-methods/mdp"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Markov Decision Process<!-- --> Â»</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-concept" class="table-of-contents__link toc-highlight">The Concept</a></li><li><a href="#exploration-vs-exploitation" class="table-of-contents__link toc-highlight">Exploration vs Exploitation</a></li><li><a href="#epsilon-greedy-strategy" class="table-of-contents__link toc-highlight">epsilon-greedy strategy</a></li></ul></div></div></div></div></main></div></div></div>
<script src="/rl-notes/assets/js/runtime~main.60cd991a.js"></script>
<script src="/rl-notes/assets/js/main.0b7ea4ad.js"></script>
</body>
</html>