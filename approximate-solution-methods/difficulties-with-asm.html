<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">Difficulties with Approximate Methods | Reinforcement Learning Notes</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://thereconpilot.github.io/rl-notes/approximate-solution-methods/difficulties-with-asm"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Difficulties with Approximate Methods | Reinforcement Learning Notes"><meta data-react-helmet="true" name="description" content="Unusual Data"><meta data-react-helmet="true" property="og:description" content="Unusual Data"><link data-react-helmet="true" rel="shortcut icon" href="/rl-notes/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://thereconpilot.github.io/rl-notes/approximate-solution-methods/difficulties-with-asm"><link data-react-helmet="true" rel="alternate" href="https://thereconpilot.github.io/rl-notes/approximate-solution-methods/difficulties-with-asm" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://thereconpilot.github.io/rl-notes/approximate-solution-methods/difficulties-with-asm" hreflang="x-default"><link rel="stylesheet" href="/rl-notes/assets/css/styles.0db5a581.css">
<link rel="preload" href="/rl-notes/assets/js/runtime~main.2af831c3.js" as="script">
<link rel="preload" href="/rl-notes/assets/js/main.b19b43fd.js" as="script">
</head>
<body>
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/rl-notes/"><div class="navbar__logo"><img src="/rl-notes/img/logo.svg" alt="Site Logo" class="themedImage_1VuW themedImage--light_3UqQ"><img src="/rl-notes/img/logo.svg" alt="Site Logo" class="themedImage_1VuW themedImage--dark_hz6m"></div><b class="navbar__title">Reinforcement Learning Notes</b></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/TheReconPilot/rl-notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_71bT toggle_3Zt9 toggleDisabled_3cF-"><div class="toggleTrack_32Fl" role="button" tabindex="-1"><div class="toggleTrackCheck_3lV7"><span class="toggleIcon_O4iE">ðŸŒœ</span></div><div class="toggleTrackX_S2yS"><span class="toggleIcon_O4iE">ðŸŒž</span></div><div class="toggleTrackThumb_xI_Z"></div></div><input type="checkbox" class="toggleScreenReader_28Tw" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_31aa"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_35hR" type="button"></button><aside class="docSidebarContainer_3Kbt"><div class="sidebar_15mo"><nav class="menu thin-scrollbar menu_Bmed"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rl-notes/">Reinforcement Learning</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Tabular Methods</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Rewards and GPI</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Model-free methods</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#">Approximate Solution Methods</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rl-notes/approximate-solution-methods/asm">Approximate Solution Methods</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/rl-notes/approximate-solution-methods/difficulties-with-asm">Difficulties with Approximate Methods</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rl-notes/approximate-solution-methods/neural-networks">Neural Networks</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rl-notes/exploration">Exploration</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rl-notes/resources">Resources</a></li></ul></nav></div></aside><main class="docMainContainer_3ufF"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><div class="tocCollapsible_1PrD theme-doc-toc-mobile tocMobile_3Hoh"><button type="button" class="clean-btn tocCollapsibleButton_2O1e">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Difficulties with Approximate Methods</h1></header><h2 class="anchor anchorWithStickyNavbar_31ik" id="unusual-data">Unusual Data<a aria-hidden="true" class="hash-link" href="#unusual-data" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_31ik" id="correlated-samples">Correlated Samples<a aria-hidden="true" class="hash-link" href="#correlated-samples" title="Direct link to heading">â€‹</a></h3><p>We could have sequences of highly correlated non-iid data. For example, consider a racing video game. We cannot simply decide what action to take just based on one image. We need multiple frames to figure out our current velocity and direction.</p><p>Much of Supervised Learning relies on the iid assumption, and when this is broken, learning can become inefficient and SGD loses convergence guarantees.</p><p><img src="https://i.imgur.com/b9J26pm.png"></p><h3 class="anchor anchorWithStickyNavbar_31ik" id="dependence-on-policy">Dependence on Policy<a aria-hidden="true" class="hash-link" href="#dependence-on-policy" title="Direct link to heading">â€‹</a></h3><p>Unseen data comes as agent learns new things. An agent could learn some fatal behaviour, or enter into some state from which it is unable to get to intended states. For example, an agent falling off the cliff.</p><h3 class="anchor anchorWithStickyNavbar_31ik" id="proximity-in-space-and-time">Proximity in space and time<a aria-hidden="true" class="hash-link" href="#proximity-in-space-and-time" title="Direct link to heading">â€‹</a></h3><p>The <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(s, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span></span> values can change abruptly in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">s</span></span></span></span></span> and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">a</span></span></span></span></span>.</p><ul><li>Close states could be arbitrarily far in value</li><li>Successive states could be arbitrarily far in value</li><li>Unstable gradients</li><li>More data needed for SGD</li></ul><p>Example, a helicopter flying very low through a forest. With careful maneuvering, it could fly just fine. But just a strike with a tree, and everything plunges into chaos.</p><hr><h2 class="anchor anchorWithStickyNavbar_31ik" id="non-stationarity">Non-Stationarity<a aria-hidden="true" class="hash-link" href="#non-stationarity" title="Direct link to heading">â€‹</a></h2><p>We cannot assume fixed training data distribution.</p><ul><li>TD targets are invalidated</li><li>MC targets no longer apply</li></ul><p>Numeric problems (oscillating behaviour)</p><ul><li>Small change in Q-values<ul><li>drastic change in policy</li><li>drastic change in training data</li><li>large gradients</li><li>large update in Q-values</li></ul></li></ul><p>The environment itself can be non-stationary.</p><hr><h2 class="anchor anchorWithStickyNavbar_31ik" id="the-deadly-triad---model-divergence">The Deadly Triad - Model Divergence<a aria-hidden="true" class="hash-link" href="#the-deadly-triad---model-divergence" title="Direct link to heading">â€‹</a></h2><ol><li>Off-policy learning<ul><li>E.g. Learning target <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ï€</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.03588em">Ï€</span></span></span></span></span> while following behaviour <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal">b</span></span></span></span></span></li></ul></li><li>Bootstrapping<ul><li>Updating a guess towards another guess (TD, DP)</li></ul></li><li>Function approximation<ul><li>Using a model with smaller number of parameters than states</li></ul></li></ol><p>These three come together and can lead the model to completely diverge, and have our agent learn absolutely nothing useful.</p><p>Divergence is not connected with Sampling, exploration or greediness.</p><hr><h2 class="anchor anchorWithStickyNavbar_31ik" id="deep-q-networks-dqn">Deep Q Networks (DQN)<a aria-hidden="true" class="hash-link" href="#deep-q-networks-dqn" title="Direct link to heading">â€‹</a></h2><p>DQN was introduced by DeepMind at Google. It is a deep convolutional network, based on Q-learning. </p><p>It was the first successful application of learning directly from raw visual inputs (same as humans), that too in a wide variety of environments (Atari games).</p><p>The video game screens were gray-scaled and downsampled to simplify computation.</p><p>There were some interesting stability tricks used in DQN, a couple of which we see here.</p><h3 class="anchor anchorWithStickyNavbar_31ik" id="experience-replay">Experience Replay<a aria-hidden="true" class="hash-link" href="#experience-replay" title="Direct link to heading">â€‹</a></h3><p>We saw experience replay earlier. We store tuples <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>A</mi><mo separator="true">,</mo><mi>R</mi><mo separator="true">,</mo><msup><mi>S</mi><mo mathvariant="normal" lspace="0em" rspace="0em">â€²</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S, A, R, S&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">â€²</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> in a pool, and we sample tuples from the pool at random, using which we update our Q-values.</p><p>This helps against correlated data, like the video game example. Computations are easy to parallelize, and so that speeds up the learning.</p><p>However, it is memory intensive. DQN stored about 1 million interactions. Random sampling could be improved. For instance, in some situations we could see that more recent data is more valuable in learning than further and older data.</p><h3 class="anchor anchorWithStickyNavbar_31ik" id="reward-clipping">Reward Clipping<a aria-hidden="true" class="hash-link" href="#reward-clipping" title="Direct link to heading">â€‹</a></h3><p>We may not know the scale of rewards beforehand. They could vary very widely, which could possibly lead to numeric problems with large Q-values.</p><p>A trick used was to clip the reward, for example, to <!-- -->[-1, 1]<!-- -->. This leads to more uniform Q-values and good gradients, however, we compromise on the distinction between <em>good</em> and <em>great</em>.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/rl-notes/approximate-solution-methods/asm"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« <!-- -->Approximate Solution Methods</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/rl-notes/approximate-solution-methods/neural-networks"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Neural Networks<!-- --> Â»</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#unusual-data" class="table-of-contents__link toc-highlight">Unusual Data</a><ul><li><a href="#correlated-samples" class="table-of-contents__link toc-highlight">Correlated Samples</a></li><li><a href="#dependence-on-policy" class="table-of-contents__link toc-highlight">Dependence on Policy</a></li><li><a href="#proximity-in-space-and-time" class="table-of-contents__link toc-highlight">Proximity in space and time</a></li></ul></li><li><a href="#non-stationarity" class="table-of-contents__link toc-highlight">Non-Stationarity</a></li><li><a href="#the-deadly-triad---model-divergence" class="table-of-contents__link toc-highlight">The Deadly Triad - Model Divergence</a></li><li><a href="#deep-q-networks-dqn" class="table-of-contents__link toc-highlight">Deep Q Networks (DQN)</a><ul><li><a href="#experience-replay" class="table-of-contents__link toc-highlight">Experience Replay</a></li><li><a href="#reward-clipping" class="table-of-contents__link toc-highlight">Reward Clipping</a></li></ul></li></ul></div></div></div></div></main></div></div></div>
<script src="/rl-notes/assets/js/runtime~main.2af831c3.js"></script>
<script src="/rl-notes/assets/js/main.b19b43fd.js"></script>
</body>
</html>