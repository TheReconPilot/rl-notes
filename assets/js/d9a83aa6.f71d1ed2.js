"use strict";(self.webpackChunkrl_notes=self.webpackChunkrl_notes||[]).push([[739],{3905:function(e,a,t){t.d(a,{Zo:function(){return o},kt:function(){return d}});var n=t(7294);function s(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){s(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function m(e,a){if(null==e)return{};var t,n,s=function(e,a){if(null==e)return{};var t,n,s={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(s[t]=e[t]);return s}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(s[t]=e[t])}return s}var l=n.createContext({}),p=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},o=function(e){var a=p(e.components);return n.createElement(l.Provider,{value:a},e.children)},c={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},u=n.forwardRef((function(e,a){var t=e.components,s=e.mdxType,r=e.originalType,l=e.parentName,o=m(e,["components","mdxType","originalType","parentName"]),u=p(t),d=s,h=u["".concat(l,".").concat(d)]||u[d]||c[d]||r;return t?n.createElement(h,i(i({ref:a},o),{},{components:t})):n.createElement(h,i({ref:a},o))}));function d(e,a){var t=arguments,s=a&&a.mdxType;if("string"==typeof e||s){var r=t.length,i=new Array(r);i[0]=u;var m={};for(var l in a)hasOwnProperty.call(a,l)&&(m[l]=a[l]);m.originalType=e,m.mdxType="string"==typeof e?e:s,i[1]=m;for(var p=2;p<r;p++)i[p]=t[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},3824:function(e,a,t){t.r(a),t.d(a,{frontMatter:function(){return m},contentTitle:function(){return l},metadata:function(){return p},toc:function(){return o},default:function(){return u}});var n=t(7462),s=t(3366),r=(t(7294),t(3905)),i=["components"],m={title:"Difficulties with Approximate Methods",sidebar_position:2},l="Difficulties with Approximate Methods",p={unversionedId:"approximate-solution-methods/difficulties-with-asm",id:"approximate-solution-methods/difficulties-with-asm",isDocsHomePage:!1,title:"Difficulties with Approximate Methods",description:"Unusual Data",source:"@site/docs/approximate-solution-methods/difficulties-with-asm.md",sourceDirName:"approximate-solution-methods",slug:"/approximate-solution-methods/difficulties-with-asm",permalink:"/rl-notes/approximate-solution-methods/difficulties-with-asm",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Difficulties with Approximate Methods",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Approximate Solution Methods",permalink:"/rl-notes/approximate-solution-methods/asm"},next:{title:"Neural Networks",permalink:"/rl-notes/approximate-solution-methods/neural-networks"}},o=[{value:"Unusual Data",id:"unusual-data",children:[{value:"Correlated Samples",id:"correlated-samples",children:[],level:3},{value:"Dependence on Policy",id:"dependence-on-policy",children:[],level:3},{value:"Proximity in space and time",id:"proximity-in-space-and-time",children:[],level:3}],level:2},{value:"Non-Stationarity",id:"non-stationarity",children:[],level:2},{value:"The Deadly Triad - Model Divergence",id:"the-deadly-triad---model-divergence",children:[],level:2},{value:"Deep Q Networks (DQN)",id:"deep-q-networks-dqn",children:[{value:"Experience Replay",id:"experience-replay",children:[],level:3},{value:"Reward Clipping",id:"reward-clipping",children:[],level:3}],level:2}],c={toc:o};function u(e){var a=e.components,t=(0,s.Z)(e,i);return(0,r.kt)("wrapper",(0,n.Z)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"difficulties-with-approximate-methods"},"Difficulties with Approximate Methods"),(0,r.kt)("h2",{id:"unusual-data"},"Unusual Data"),(0,r.kt)("h3",{id:"correlated-samples"},"Correlated Samples"),(0,r.kt)("p",null,"We could have sequences of highly correlated non-iid data. For example, consider a racing video game. We cannot simply decide what action to take just based on one image. We need multiple frames to figure out our current velocity and direction."),(0,r.kt)("p",null,"Much of Supervised Learning relies on the iid assumption, and when this is broken, learning can become inefficient and SGD loses convergence guarantees."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://i.imgur.com/b9J26pm.png",alt:null})),(0,r.kt)("h3",{id:"dependence-on-policy"},"Dependence on Policy"),(0,r.kt)("p",null,"Unseen data comes as agent learns new things. An agent could learn some fatal behaviour, or enter into some state from which it is unable to get to intended states. For example, an agent falling off the cliff."),(0,r.kt)("h3",{id:"proximity-in-space-and-time"},"Proximity in space and time"),(0,r.kt)("p",null,"The ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mi",{parentName:"mrow"},"Q"),(0,r.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,r.kt)("mi",{parentName:"mrow"},"s"),(0,r.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,r.kt)("mi",{parentName:"mrow"},"a"),(0,r.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"Q(s, a)")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"Q"),(0,r.kt)("span",{parentName:"span",className:"mopen"},"("),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,r.kt)("span",{parentName:"span",className:"mpunct"},","),(0,r.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.16666666666666666em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"a"),(0,r.kt)("span",{parentName:"span",className:"mclose"},")")))))," values can change abruptly in ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mi",{parentName:"mrow"},"s")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"s")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.43056em",verticalAlign:"0em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"s")))))," and ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mi",{parentName:"mrow"},"a")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"a")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.43056em",verticalAlign:"0em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"a"))))),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Close states could be arbitrarily far in value"),(0,r.kt)("li",{parentName:"ul"},"Successive states could be arbitrarily far in value"),(0,r.kt)("li",{parentName:"ul"},"Unstable gradients"),(0,r.kt)("li",{parentName:"ul"},"More data needed for SGD")),(0,r.kt)("p",null,"Example, a helicopter flying very low through a forest. With careful maneuvering, it could fly just fine. But just a strike with a tree, and everything plunges into chaos."),(0,r.kt)("hr",null),(0,r.kt)("h2",{id:"non-stationarity"},"Non-Stationarity"),(0,r.kt)("p",null,"We cannot assume fixed training data distribution."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"TD targets are invalidated"),(0,r.kt)("li",{parentName:"ul"},"MC targets no longer apply")),(0,r.kt)("p",null,"Numeric problems (oscillating behaviour)"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Small change in Q-values",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"drastic change in policy"),(0,r.kt)("li",{parentName:"ul"},"drastic change in training data"),(0,r.kt)("li",{parentName:"ul"},"large gradients"),(0,r.kt)("li",{parentName:"ul"},"large update in Q-values")))),(0,r.kt)("p",null,"The environment itself can be non-stationary."),(0,r.kt)("hr",null),(0,r.kt)("h2",{id:"the-deadly-triad---model-divergence"},"The Deadly Triad - Model Divergence"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Off-policy learning",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"E.g. Learning target ",(0,r.kt)("span",{parentName:"li",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mi",{parentName:"mrow"},"\u03c0")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\pi")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.43056em",verticalAlign:"0em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"\u03c0")))))," while following behaviour ",(0,r.kt)("span",{parentName:"li",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mi",{parentName:"mrow"},"b")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"b")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"0.69444em",verticalAlign:"0em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"b")))))))),(0,r.kt)("li",{parentName:"ol"},"Bootstrapping",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Updating a guess towards another guess (TD, DP)"))),(0,r.kt)("li",{parentName:"ol"},"Function approximation",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Using a model with smaller number of parameters than states")))),(0,r.kt)("p",null,"These three come together and can lead the model to completely diverge, and have our agent learn absolutely nothing useful."),(0,r.kt)("p",null,"Divergence is not connected with Sampling, exploration or greediness."),(0,r.kt)("hr",null),(0,r.kt)("h2",{id:"deep-q-networks-dqn"},"Deep Q Networks (DQN)"),(0,r.kt)("p",null,"DQN was introduced by DeepMind at Google. It is a deep convolutional network, based on Q-learning. "),(0,r.kt)("p",null,"It was the first successful application of learning directly from raw visual inputs (same as humans), that too in a wide variety of environments (Atari games)."),(0,r.kt)("p",null,"The video game screens were gray-scaled and downsampled to simplify computation."),(0,r.kt)("p",null,"There were some interesting stability tricks used in DQN, a couple of which we see here."),(0,r.kt)("h3",{id:"experience-replay"},"Experience Replay"),(0,r.kt)("p",null,"We saw experience replay earlier. We store tuples ",(0,r.kt)("span",{parentName:"p",className:"math math-inline"},(0,r.kt)("span",{parentName:"span",className:"katex"},(0,r.kt)("span",{parentName:"span",className:"katex-mathml"},(0,r.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,r.kt)("semantics",{parentName:"math"},(0,r.kt)("mrow",{parentName:"semantics"},(0,r.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,r.kt)("mi",{parentName:"mrow"},"S"),(0,r.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,r.kt)("mi",{parentName:"mrow"},"A"),(0,r.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,r.kt)("mi",{parentName:"mrow"},"R"),(0,r.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,r.kt)("msup",{parentName:"mrow"},(0,r.kt)("mi",{parentName:"msup"},"S"),(0,r.kt)("mo",{parentName:"msup",mathvariant:"normal",lspace:"0em",rspace:"0em"},"\u2032")),(0,r.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,r.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"(S, A, R, S')")))),(0,r.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,r.kt)("span",{parentName:"span",className:"base"},(0,r.kt)("span",{parentName:"span",className:"strut",style:{height:"1.001892em",verticalAlign:"-0.25em"}}),(0,r.kt)("span",{parentName:"span",className:"mopen"},"("),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05764em"}},"S"),(0,r.kt)("span",{parentName:"span",className:"mpunct"},","),(0,r.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.16666666666666666em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal"},"A"),(0,r.kt)("span",{parentName:"span",className:"mpunct"},","),(0,r.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.16666666666666666em"}}),(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.00773em"}},"R"),(0,r.kt)("span",{parentName:"span",className:"mpunct"},","),(0,r.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.16666666666666666em"}}),(0,r.kt)("span",{parentName:"span",className:"mord"},(0,r.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05764em"}},"S"),(0,r.kt)("span",{parentName:"span",className:"msupsub"},(0,r.kt)("span",{parentName:"span",className:"vlist-t"},(0,r.kt)("span",{parentName:"span",className:"vlist-r"},(0,r.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.751892em"}},(0,r.kt)("span",{parentName:"span",style:{top:"-3.063em",marginRight:"0.05em"}},(0,r.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,r.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,r.kt)("span",{parentName:"span",className:"mord mtight"},(0,r.kt)("span",{parentName:"span",className:"mord mtight"},"\u2032"))))))))),(0,r.kt)("span",{parentName:"span",className:"mclose"},")")))))," in a pool, and we sample tuples from the pool at random, using which we update our Q-values."),(0,r.kt)("p",null,"This helps against correlated data, like the video game example. Computations are easy to parallelize, and so that speeds up the learning."),(0,r.kt)("p",null,"However, it is memory intensive. DQN stored about 1 million interactions. Random sampling could be improved. For instance, in some situations we could see that more recent data is more valuable in learning than further and older data."),(0,r.kt)("h3",{id:"reward-clipping"},"Reward Clipping"),(0,r.kt)("p",null,"We may not know the scale of rewards beforehand. They could vary very widely, which could possibly lead to numeric problems with large Q-values."),(0,r.kt)("p",null,"A trick used was to clip the reward, for example, to ","[-1, 1]",". This leads to more uniform Q-values and good gradients, however, we compromise on the distinction between ",(0,r.kt)("em",{parentName:"p"},"good")," and ",(0,r.kt)("em",{parentName:"p"},"great"),"."))}u.isMDXComponent=!0}}]);