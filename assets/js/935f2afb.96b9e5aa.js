"use strict";(self.webpackChunkrl_notes=self.webpackChunkrl_notes||[]).push([[53],{1109:function(e){e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Reinforcement Learning","href":"/rl-notes/"},{"type":"category","label":"Tabular Methods","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Multi-Armed Bandits","href":"/rl-notes/tabular-methods/multi-armed-bandits"},{"type":"link","label":"Markov Decision Process","href":"/rl-notes/tabular-methods/mdp"},{"type":"link","label":"Cross Entropy Method","href":"/rl-notes/tabular-methods/cross-entropy"}]},{"type":"category","label":"Rewards and GPI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Reward Design","href":"/rl-notes/dynamic-programming/reward-design"},{"type":"link","label":"Value Functions","href":"/rl-notes/dynamic-programming/value-functions"},{"type":"link","label":"Optimality","href":"/rl-notes/dynamic-programming/optimality"},{"type":"link","label":"Generalized Policy Iteration","href":"/rl-notes/dynamic-programming/gpi"}]},{"type":"category","label":"Model-free methods","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Monte-Carlo Methods","href":"/rl-notes/model-free-methods/monte-carlo"},{"type":"link","label":"Temporal Difference","href":"/rl-notes/model-free-methods/temporal-difference"}]},{"type":"link","label":"Resources","href":"/rl-notes/resources"}]}}')}}]);